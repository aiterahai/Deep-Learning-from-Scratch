# Deep Learning from Scratch

## Perceptron

​	**퍼셉트론**은 **신경망**의 기원이 되는 알고리즘입니다. 

* **퍼셉트론이란?**

  퍼셉트론은 **다수**의 **신호**를 입력으로 받아 하나의 **신호**를 출력하는 것 입니다. 

  퍼셉트론 **신호**는 **흐름**을 만들고 정보를 앞으로 전달합니다. 

  퍼셉트론은 **1과 0** 즉 **신호가 흐른다/ 신호가 흐르지 않는다**로 **두 가지 값**을 가질 수 있습니다.

![01. 퍼셉트론 - Perceptron](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99BDCE4D5B98A1022C)

​	위의 그림은 **입력**으로 2개의 신호를 받은 퍼셉트론의 예입니다.

​	x_1과 x_2는 **입력 신호**, y는 **출력 신호**, w_1과 w_2는 **가중치**를 뜻합니다.

​	그림의 원을 **뉴런** 혹은 **노드**라고 부릅니다.

​	입력 신호가 뉴런에 보내질 때는 각각 고유한 **가중치**가 곱해집니다. (x_1w_1, x_2w_2)

​	뉴런에서 보내온 신호의 총합이 **정해진 한계**를 넘어설 때만 1을 출력합니다.

​	그 한계를 **임계값**이라 하며 θ 기호로 나타냅니다.

​	이 것을 수식으로 나타내면 아래의 수식이 됩니다.
$$
y =
\begin{cases}
0\quad  (w_1x_1\ +\ w_2x_2 \leθ)\\
1\quad  (w_1x_1\ +\ w_2x_2 >θ)
\end{cases}
$$
​	퍼셉트론은 복수의 입력 신호 각각에 고유한 **가중치**를 부여합니다.

​	**가중치**는 각 신호가 결과에 주는 **영향력을 조절하는 요소**로 작용합니다.

​	**가중치**가 클수록 해당 신호가 그만큼 **더 중요함**을 뜻합니다.

* **논리 회로**

  * **AND 게이트**

    AND 게이트는 입력이 둘이고 출력은 하나입니다.

    아래 그림 처럼 입력 신호와 출력 신호의 대응 표를 **진리표**라고 합니다.

    이 그림은 AND 게이트의 진리표로, 두 입력이 모두 1일 때만 1을 출력하고, 그 외에는 0을 출력합니다.

    | x_1  | x_2  | y    |
    | ---- | ---- | ---- |
    | 0    | 0    | 0    |
    | 0    | 1    | 0    |
    | 1    | 0    | 0    |
    | 1    | 1    | 1    |

    이 AND 게이트를 퍼셉트론으로 표현을 할 것 입니다.

    우선 진리표대로 작동하도록 하는 **w_1, w_2, θ**의 값을 정해야 합니다.

    위의 진리표를 만족하는 **매개변수** 조합은 **무한**히 많습니다.

    (w_1, w_2, θ)가 (0.5, 0.5, 0.7) 일 때, 또 (0.5, 0.5, 0.8)이나 (1.0, 1.0, 1.0) 일때 모두 **AND 게이트의 조건을 만족합니다.**

    매개변수를 이렇게 설정하면 x_1과 x_2가 모두 1일 때만 가중 신호의 총합이 주어진 **임계 값**을 웃돌게 됩니다.

  * **NAND 게이트와 OR 게이트**

    NAND 는 Not AND를 의미합니다.

    그 동작은 AND 게이트의 출력을 **뒤집은 것**이 됩니다.

    | x_1  | x_2  | y    |
    | ---- | ---- | ---- |
    | 0    | 0    | 1    |
    | 0    | 1    | 1    |
    | 1    | 0    | 1    |
    | 1    | 1    | 0    |

    진리표로 나타내면 x_1와 x_2가 모두 1일 때만 0을 출력하고 그 이외의 경우에는 1을 출력합니다.

    NAND 게이트를 표현하려면 다음과 같은 조합이 있습니다.
    $$
    (w_1, w_2, θ) = (-0.5, -0.5, -0.7)
    $$
    AND 게이트를 구현하는 **매개변수의 부호를 모두 반전**하기만 하면 NAND의 게이트가 됩니다.
    
    OR 게이트는 입력 신호 중 하나 이상이 1이면 출력이 1이 되는 논리 회로입니다.
    
    | x_1  | x_2  | y    |
    | ---- | ---- | ---- |
    | 0    | 0    | 0    |
    | 0    | 1    | 1    |
    | 1    | 0    | 1    |
    | 1    | 1    | 1    |
    
    OR 게이트를 표현하라면 다음과 같은 조합이 있습니다.
    $$
    (w_1, w_2, θ) = (0.5, 0.5, 0.3)
    $$

  이것들 처럼 퍼셉트론으로 AND, NAND, OR 과 같은 논리 회로를 표현할 수 있습니다.

  여기서 중요한 점은 퍼셉트론의 구조는 AND, NAND, OR 게이트 모두에서 똑같다는 것 입니다.

  세 가지 게이트에서 다른 것은 매개변수의 값뿐입니다.

  즉 똑같은 구조의 퍼셉트론이 매개변수의 값만 적절히 조정하여 AND, NAND, OR로 변신 할 수 있습니다.

  

* **퍼셉트론 구현하기**

  * **AND 게이트**

    ![image-20210914210504078](Image\AND.png)

    다음은 x1과 x2를 매개변수로 받는 AND라는 함수입니다.

    매개변수 w1, w2, theta는 함수 안에서 초기화하고, 가중치를 곱한 입력의 총합이 임계값을 넘으면 1을 반환하고 그 외에는 0을

    반환합니다.

    위 그림 처럼 AND 가 잘 작동하는 것을 볼 수 있습니다.

  * **가중치와 편향**

    전에 봤던 아래의 식에서
    $$
    y =
    \begin{cases}
    0\quad  (w_1x_1\ +\ w_2x_2 \leθ)\\
    1\quad  (w_1x_1\ +\ w_2x_2 >θ)
    \end{cases}
    $$
    θ를 -b로 치환하면 퍼셉트론의 동작이 아래 식처럼 됩니다.
    $$
    y =
    \begin{cases}
    0\quad  (b\ +\ w_1x_1\ +\ w_2x_2 \le0)\\
    1\quad  (b\ +\ w_1x_1\ +\ w_2x_2 >0)
    \end{cases}
    $$
    

    두개의 식은 기호만 바꿧을 뿐, 의미는 같습니다.

    여기서 b를 **편향 Bias**이라고 하고 w_1, w_2는 그대로 가중치 입니다.

    ![image-20210914212906289](Image\AND_Bias.png)

    여기에서 -θ가 편향 b로 치환이 되었습니다.

    그리고 편향은 가중치와 기능이 다르다는 사실에 주의해야 합니다.

    가중치는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수이고, 편향은 뉴런이 얼마나 쉽게 활성화 하느냐를

    조정하는 매개변수입니다.

  * NAND

    ![image-20210914215034074](Image\NAND.png)

    NAND는 AND의 가중치와 편향의 부호를 바꾸어 주면 됩니다.

  * OR

    ![image-20210914215814518](Image\OR.png)

    OR 게이트는 AND 게이트의 가중치와 편향만 바꾸어주면 됩니다.

  이처럼 AND, NAND, OR의 게이트는 모두 가중치와 편향만 바꾸어주면 다른 게이트를 만들어 줄 수 있습니다.

* **퍼셉트론의 한계**

  지금까지 퍼셉트론을 이용해서 AND, NAND, OR의 3가지 논리 회로를 구현 해보았습니다.

  * **XOR 게이트**

    XOR 게이트는 **베타적 논리합**이라는 논리 회로 입니다. 

    x_1과 x_2중 한쪽이 1일 때만 1을 출력합니다.

    | x_1  | x_2  | y    |
    | ---- | ---- | ---- |
    | 0    | 0    | 0    |
    | 0    | 1    | 1    |
    | 1    | 0    | 1    |
    | 1    | 1    | 0    |

    지금 까지 배운 퍼셉트론으로는 XOR 게이트를 구현할 수 없습니다.

    ![img](https://t1.daumcdn.net/cfile/tistory/992152485B98A1C705)

    위 그림과 같이 퍼셉트론은 **직선**으로 나뉜 두 영역을 만듭니다.

  * **선형과 비선형**

    만약 퍼셉트론이 직선 즉 선형이라는 제약을 없엔 **비선형**구조 라면, 아래 그림과 같이 영역을 나눌 수 있습니다.

    ![img](https://t1.daumcdn.net/cfile/tistory/991D844D5B98A1DE08)

* **다층 퍼셉트론**

  * **게이트 조합**

    ![img](https://t1.daumcdn.net/cfile/tistory/995729435B98A1FE3D)

    NAND게이트와 OR게이트와 AND게이트로 XOR게이트를 만들 수 있습니다.

  * **XOR 게이트 구현**

    ![image-20210914221845851](Image\XOR.png)

    이 것을 퍼셉트론으로 나타내면 아래 그림 처럼 됩니다.

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/754c7c20-3a21-11ea-8734-d1dac55eae87/2-13XOR%EC%9D%98-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.png)

    AND, OR이 단층 퍼셉트론 인데 반해 XOR은 2층 퍼셉트론입니다.

    따라서 이처럼 층이 여러 개인 퍼셉트론을 다층 퍼셉트론이라고 합니다.

* **정리**

  퍼셉트론은 입출력을 갖춘 알고리즘입니다. 입력을 주면 정해진 규칙에 따른 값을 출력합니다.

  퍼셉트론은 가중치와 편향을 매개변수로 설정합니다.

  퍼셉트론으로 AND, OR 게이트 등의 논리 회로를 표현할 수 있습니다.

  XOR 게이트는 단층 퍼셉트론으로는 표현할 수 없습니다.

  2층 퍼셉트론을 이용하면 XOR 게이트를 표현할 수 있습니다.

  단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현할 수 있습니다.

  다층 퍼셉트론은 이론상 컴퓨터를 표현할 수 있습니다.



## 신경망

* **신경망이란?**

  ![img](https://media.vlpt.us/post-images/dscwinterstudy/82dcf020-38f2-11ea-98c4-63cdb84222e2/%EC%8B%A0%EA%B2%BD%EB%A7%9D.png)

  입력층이나 출력층과 달리 은닉층의 뉴런은 사람의 눈에 보이지 않습니다.

  0층을 입력층, 1층을 은닉층, 2층을 출력층이라고 합니다.

  위의 그림 속 신경망은 3층으로 구성되어있지만 가중치를 갖는 층이 2개이기 때문에 **2층 신경망**이라고 합니다.

  활성화 함수 Activation function :

  * 입력 신호의 총합을 출력 신호로 변환하는 함수입니다.
  * 변환된 신호를 다음 뉴런에 전달합니다.
  * 입력 신호의 총합이 활성화를 일으키는지를 정하는 역활을 합니다.

  ![img](https://media.vlpt.us/post-images/dscwinterstudy/d1593000-38e9-11ea-b942-cb9b82d31200/%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98%EC%9D%98-%EC%B2%98%EB%A6%AC-%EA%B3%BC%EC%A0%95.PNG)

  > a = b+w1x1+w2x2 
  > y=h(a) A를 함수 h()에 넣어서 y를 출력합니다.

  단순 퍼셉트론 : 단층 네트워크에서 계단 함수를 활성화 함수로 사용한 모델입니다.

  다층 퍼셉트론 : 신경망 즉 여러 층으로 구성되어있고, Sigmoid 함수 등의 매끈한 활성화 함수를 사용하는 네트워크 입니다.

* **활성화 함수의 종류**

  * **Sigmoid function**

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/a905ddd0-3829-11ea-8217-2f54dec14d43/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98.PNG)

    시그모이드 함수는 출력 값의 범위가 0~1 사이이며, 매우 매끄러운 곡선을 가집니다.

    분류는 0과 1로 나뉘며, 출력 값이 어느 값에 가까운지를 통해 어느 분류에 속하는지 쉽게 알 수 있습니다.

    

    ![image-20210915223034974](Image\Sigmoid.png)

  * **Step function**

    ![image-20210915223337368](Image\Step.png)

    계단 함수는 0과 1중 하나의 값만 돌려줍니다.

    하지만 시그모이드 함수는 0과 1사이의 연속적인 실수를 돌려줍니다.

    * **계단 함수와 시그모이드 함수의 공통점**

      입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력합니다.

      출력은 0과 1사이 입니다.

      비선형 함수입니다.

      > **선형함수의 문제점**
      >
      > * 층을 아무리 깊게 해도 은닉층이 없는 네트워크로도 똑같은 기능을 할 수 있습니다.
      >
      >   선형 함수로 레이어를 쌓는다면 여러 층으로 구성하는 이점을 살릴 수 없습니다.

    * **계단 함수와 시그모이드 함수의 차이점**

      계단 함수는 0과 1중 하나의 값만을 돌려주지만

      시그모이드는 연속적인 실수를 돌려주어서 확률, 통계적으로 사용할 수 있습니다.

  * **ReLU function**

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/c627e7a0-3829-11ea-8bae-8b3dd278412c/ReLU%ED%95%A8%EC%88%98.PNG)

    ReLU 함수는 입력이 0이 넘으면 그 입력을 그대로 출력하고 , 0 이하이면 0 을 출력하는 함수 입니다.

    ![image-20210915224017264](Image\ReLU.png)

    학습이 매우 빠르고, 연산 비용이 적고, 구현이 매우 간단하다는 장점이 있습니다.

    하지만 0보다 작은 값들에서 뉴런이 죽을 수 있다는 단점이 있습니다.

* **다차원 배열의 계산**

  * **행렬의 곱**

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/11890d50-38fc-11ea-8a14-7ddb1d5b4b35/%ED%96%89%EB%A0%AC%EA%B3%B1.png)

    ![image-20210915225732083](Image\np_2.png)

    np.dot() 행렬 곱을 계산합니다.

    np.dot(A, B)와 np.dot(B, A)는 다른 값이 될 수 있습니다.
    
    ![img](https://media.vlpt.us/post-images/dscwinterstudy/bd94fa40-38fd-11ea-8a14-7ddb1d5b4b35/%ED%96%89%EB%A0%AC%ED%98%95%EC%83%81%EA%B3%B1.png)
    
    행렬A의 1번째 차원의 원소 수 와 행렬B의 0번째 차원의 원소 수가 같아야합니다.
    
    ![image-20210916230725737](Image\np_3.png)
    
  * **신경망에서의 행렬의 곱**
  
    ![img](https://media.vlpt.us/post-images/dscwinterstudy/54edf130-38fe-11ea-8a14-7ddb1d5b4b35/%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%96%89%EB%A0%AC%EA%B3%B1.png)
  
    ![image-20210916230938515](Image\np_4.png)
  
* **3층 신경망 구현**

  * **표기법**

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/f88732c0-38fe-11ea-b428-5dc446614305/%EA%B0%80%EC%A4%91%EC%B9%98%ED%91%9C%EA%B8%B0.png)

  * **각 층의 신호 전달 구현**

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/4f6a4300-39fb-11ea-b6b9-5148bcc9f2c4/%EC%9E%85%EB%A0%A5%EC%B8%B5%EC%97%90%EC%84%9C1%EC%B8%B5%EB%B3%B4%EC%B6%A9.png)

    은닉층에서의 가중치의 합을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표현합니다.

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/b18ace60-39fb-11ea-b6b9-5148bcc9f2c4/a1.png)

    1층의 가중치 부분을 행렬식으로 나타낸다면 아래 그림처럼 됩니다.

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/70bc13d0-39fb-11ea-b6b9-5148bcc9f2c4/1%EC%B8%B5%EA%B0%80%EC%A4%91%EC%B9%98%ED%96%89%EB%A0%AC%EC%8B%9D.png)

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/7bede9c0-39fd-11ea-9694-9dbcffa449db/%EA%B0%80%EC%A4%91%EC%B9%98%ED%96%89%EB%A0%AC.PNG)

    ![image-20210916231337228](Image\NN_1.png)

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/f77f36c0-39fd-11ea-9304-8d735a4c8d2b/1%EC%B8%B5%EC%97%90%EC%84%9C-2%EC%B8%B5%EC%9C%BC%EB%A1%9C.png)

    ![image-20210916231432331](Image\NN_2.png)

    ![img](https://media.vlpt.us/post-images/dscwinterstudy/e3844950-3a00-11ea-85c7-af1be234c277/%EC%B6%9C%EB%A0%A5%EC%B8%B5%EC%9C%BC%EB%A1%9C-%EC%8B%A0%ED%98%B8-%EC%A0%84%EB%8B%AC.png)

    ![image-20210916231530378](Image\NN_3.png)

    일반적으로 회귀에서는 출력층의 활성화 함수를 항등함수로, 바이너리 클래스 분류에서는 시그모이드 함수를, 다중 클래스 분류에서는 소프트맥스 함수를

    사용한다고 합니다.

    

  * **3층 신경망 구현**

    ![image-20210916231741342](Image\NN_4.png)

* **출력층 설계**

  * **항등 함수와 소프트맥스 함수**

    - 항등함수(identity function): 입력을 그대로 출력

      ![img](https://media.vlpt.us/post-images/dscwinterstudy/98bd1470-3a04-11ea-a976-bbc34e4880b0/%ED%95%AD%EB%93%B1%ED%95%A8%EC%88%98.png)

    - 소프트맥스 함수(softmax function):

      ![img](https://media.vlpt.us/post-images/dscwinterstudy/a70d9ad0-382a-11ea-83b1-3b44e26c4216/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG)

      소프트맥스 함수의 분자는 입력 신호의 지수 함수, 분모는 모든 입력 신호의 지수 함수의 합으로 이루어져 있습니다.\

      ```python
      def softmax(a):
        exp_a = np.exp(a)
        sum_exp_a = np.sum(exp_a)
        y = exp_a / sum_exp_a
        
        return y
      ```

      ![img](https://media.vlpt.us/post-images/dscwinterstudy/8c838090-3a04-11ea-a976-bbc34e4880b0/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98.png)

      * **소프트맥스 함수 구현의 주의점**

        소프트맥스는 지수함수를 사용하기 때문에, Overflow의 문제가 발생해서 수치가 불안정해질 수 있다는 문제점이 있습니다.

        ![img](https://media.vlpt.us/post-images/dscwinterstudy/7fbb6850-3a04-11ea-a976-bbc34e4880b0/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%ED%95%A8%EC%88%98%EC%88%98%EC%A0%95.png)

        소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더 하거나 빼도 결과는 바뀌지 않는다는 특성을 이용해 식을 수정합니다,

        오버플로우를 막기위해 C에 최댓값을 넣는게 일반적입니다.

        ```python
        def softmax(a):
          c = np.max(a)
          exp_a = np.exp(a-c)
          sum_exp_a = np.sum(exp_a)
          y = exp_a / sum_exp_a
          
          return y
        ```

      * **소프트맥스 함수의 특징**

        * 출력값은 0과 1사이의 실수입니다.

        * 출력의 총합이 1입니다. (확률로 해석할 수 있습니다.)

        * 지수함수가 단조 증가 함수 이기 때문에 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않습니다.

          따라서 신경망으로 분류할 때에 출력층의 소프트맥스 함수를 생략해도 됩니다.

      * **출력층의 뉴런 수**

        보통 분류 문제 에서는 분류하고 싶은 클래스 수로 뉴런 수를 설정하는 것이 일반적입니다.

* **MNIST 숫자 인식**